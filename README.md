# Project Overview : 
This project is my own implementation of a robust end-to-end ELT (Extract, Load, Transform) pipeline designed to efficiently process large volumes of daily sales data batches generated by multiple retail servers. These batches arrive in various raw formats such as JSON, text, and CSV. The primary goal of this pipeline is to automate the ingestion and transformation of disparate raw data into consistent, clean, and business-ready data models and data marts that can be immediately used for OLAP (Online Analytical Processing) or for further data science workloads.

## key features :
- Support for multiple input data formats and flexible parsing logic.
- Scalable batch processing architecture for handling large volumes of data daily.
-Frequent data quality checks and error handling mechanism.
-optimized for OLAP queries and data analysis workflows.
## Tools Used:
- **Docker** – Containerization  
- **Airflow** – Workflow orchestration  
- **DBT Core** – SQL-based data transformations  
- **Soda Core** – Data quality checks  
- **Snowflake** – Cloud data warehouse  
- **Snowsight** – Built-in data visualization tool in Snowflake  
- **Python** – Scripting and extraction logic  


# Pipeline Architecture : 

![Retail-pipeline-diagram](https://github.com/user-attachments/assets/e2f1524f-b530-4d65-b0ff-5c290c7f3ba5)

## Data Source

The pipeline begins with a continuous flow of simulated sales data batches, representing data generated by various retail servers. To create realistic and diverse inputs, a custom Python script was developed. This script first **samples subsets from a large historical sales DataFrame** using random sampling techniques to generate base batches. Then, it applies the **SMOTE (Synthetic Minority Over-sampling Technique)** algorithm to selectively modify certain attributes, introducing synthetic variability and improving class balance in the data.


## Data Lake  
The data lake is implemented using **S3 bucket storage**, where incoming batches from various retail servers are stored. Each file is timestamped to reflect its ingestion time and can exist in JSON, text, or CSV format.


## Load  
The loading phase is triggered daily at the start of the day. It performs the following tasks:
- Reads and consolidates the raw batch files into a unified CSV format  
- Loads the structured raw data into a Snowflake stage  
This stage acts as a temporary holding zone before any transformation or cleansing is applied.

## Transform
Once the data is staged in Snowflake, it undergoes a multi-step transformation process categorized into Daily and Weekly jobs:
### Daily:
- Data is loaded into a raw table in Snowflake, then undergoes a series of cleaning and transformation steps using dbt queries. These steps include standardizing formats, filling null values, normalizing data, and finally loading the cleaned data into an incremental table that stores the processed results.

-After the cleaning process, a set of data quality checks is executed on the recently cleaned data to ensure it meets the Silver Layer standards. These checks validate that there are no duplicates, nulls are properly handled, and all column formats and names are correct. The data validation is performed using the Soda Core framework.

- Once data is validated, it is then reshaped into a star schema using another incremental DBT models having:

Central fact tables (ex: sales)

Multiple dimension tables (ex: products,countries,stores)

### Weekly:
- At the beginning of each week, a final round of data quality checks is executed before building the aggregation models. These validations use custom Soda test queries to ensure the dataset conforms to Gold Layer standards. Checks include:

Primary key uniqueness

Foreign key integrity

Business rule validations

-The previously validated star schema data is used to create simple data marts optimized for OLAP (Online Analytical Processing) and reporting.These marts are visualized using Snowsight a built-in data visualization tool within Snowflake.

## Containerization And Orcherstartion :
The whole pipeline is orchestrated using Airflow DAGs, each responsible for a sequence of tasks.The entire orchestrated pipeline is packaged as a single Docker container for easier deployment,reproducibility, and better future scalability.



# Repo's Structure :
```
Airflow/
├── dags/
└── include/
    │
    │
    ├── dbt/
    │   └── retail_pipeline_dbt_project/
    │       ├── analyses/
    │       ├── logs/
    │       ├── macros/
    │       ├── models/
    │       │   ├── marts/
    │       │   ├── star-wh/
    │       │   └── transformed/
    │       ├── seeds/
    │       ├── snapshots/
    |       └── tests/
    │       
    └── soda/
        └── retail_pipeline_soda_project/
            └── checks/
```
 `./dags`:  
  Folder containing all DAG Python files used for pipeline scheduling.  
  *(Only daily run DAGs are included in the repo for simplicity, as the weekly runs have very similar logic)*

- `./include`:  
  Contains all auxiliary folders/files used by the DAG scripts, including:  
  - `./include/dbt/retail_pipeline_dbt_project`:  
    The dbt project folder containing all incremental/views data model queries, macros, and other resources run by the DAG scripts on a daily or weekly basis.  
  - `./include/soda/retail_pipeline_soda_project`:  
    The Soda project folder holding some of the test YAML files used in the project.  
    *(Only simple data quality checks are included in this repo)*






