# Project Overview : 
This project is my own implementation of a robust end-to-end ELT (Extract, Load, Transform) pipeline designed to efficiently process large volumes of daily sales data batches generated by multiple retail servers. These batches arrive in various raw formats such as JSON, text, and CSV. The primary goal of this pipeline is to automate the ingestion and transformation of disparate raw data into consistent, clean, and business-ready data models and data marts that can be immediately used for OLAP (Online Analytical Processing) or for further data science workloads.

## key features :
- Support for multiple input data formats and flexible parsing logic.
- Scalable batch processing architecture for handling large volumes of data daily.
-Frequent data quality checks and error handling mechanism.
-optimized for OLAP queries and data analysis workflows.
## Tools Used:
- Docker
- Airflow
- Dbt-Core
- Soda-Core
- Snowflake
- Snowsight
- Python


# Pipeline Architecture : 

![Retail-pipeline-diagram](https://github.com/user-attachments/assets/e2f1524f-b530-4d65-b0ff-5c290c7f3ba5)

## Data Source

The pipeline begins with a continuous flow of simulated sales data batches, representing data generated by various retail servers. To create realistic and diverse inputs, a custom Python script was developed. This script first **samples subsets from a large historical sales DataFrame** using random sampling techniques to generate base batches. Then, it applies the **SMOTE (Synthetic Minority Over-sampling Technique)** algorithm to selectively modify certain attributes, introducing synthetic variability and improving class balance in the data.


## Data Lake  
The data lake is implemented using **S3 bucket storage**, where incoming batches from various retail servers are stored. Each file is timestamped to reflect its ingestion time and can exist in JSON, text, or CSV format.


## Load  
The loading phase is triggered daily at the start of the day. It performs the following tasks:
- Reads and consolidates the raw batch files into a unified CSV format  
- Loads the structured raw data into a Snowflake stage  
This stage acts as a temporary holding zone before any transformation or cleansing is applied.


## Transform  
Once raw data is staged in Snowflake, the transformation phase is managed using **dbt-core**. Key tasks include:
- Cleansing and standardizing data (e.g., handling missing values, type casting)  
- Creating normalized **dimension** and **fact** tables based on business rules  
- Building analytical **data marts** using star or snowflake schemas  
- Tracking transformations using dbtâ€™s model documentation and lineage features  

These transformations ensure that data is reliable, consistent, and query-optimized for downstream use.

## Data Quality Checks  
Before loading into final marts, **Soda-core** performs automated data validation:

- Schema checks and null constraints    
- Business rule validations (e.g., prices must be non-negative)  

Failures can trigger alerts or halt the pipeline, ensuring data trust and integrity.



